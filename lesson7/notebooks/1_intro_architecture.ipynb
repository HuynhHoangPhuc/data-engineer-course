{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7: Architecture Overview & Demo\n",
    "\n",
    "Welcome to Lesson 7! In this notebook, we will explore the architecture of our Big Data platform and run a simple end-to-end data pipeline.\n",
    "\n",
    "## 1. Architecture Overview\n",
    "\n",
    "Our platform consists of several key components working together:\n",
    "\n",
    "1.  **PostgreSQL (`postgres`)**: Our relational database source, hosting the `pagila` (DVD rental) dataset.\n",
    "2.  **Apache Spark (`spark-master`, `spark-worker`)**: The distributed processing engine used for ETL (Extract, Transform, Load).\n",
    "3.  **MinIO (`minio`)**: An S3-compatible object storage serving as our Data Lake (Bronze, Silver, Gold layers).\n",
    "4.  **Trino (`trino`)**: A distributed SQL query engine for analytics across our data sources.\n",
    "5.  **Jupyter (`jupyter`)**: This environment, used for development and exploration.\n",
    "\n",
    "### Data Flow Diagram\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Postgres (Source)] -->|Extract| B(Spark Engine)\n",
    "    B -->|Load Raw| C[MinIO Bronze]\n",
    "    C -->|Read| B\n",
    "    B -->|Transform| D[MinIO Silver]\n",
    "    D -->|Read| E[Trino]\n",
    "    E -->|Analyze| F[Jupyter/SQL Client]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "First, we need to initialize our Spark session with the necessary configurations to communicate with MinIO (via S3A) and Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lesson7-Architecture-Demo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.5.4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step 1: Extract (Postgres to Spark)\n",
    "\n",
    "We will read the `customer` table from the `pagila` database currently running in our Postgres container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postgres Connection Details\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/pagila\"\n",
    "db_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Read data from Postgres\n",
    "df_customer = spark.read.jdbc(url=jdbc_url, table=\"customer\", properties=db_properties)\n",
    "\n",
    "print(f\"Extracted {df_customer.count()} records from Postgres.\")\n",
    "df_customer.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step 2: Load (Spark to MinIO - Bronze)\n",
    "\n",
    "Now we save this raw data into our Data Lake (MinIO) in the `bronze` bucket. We'll use the Parquet format, which is optimized for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 path\n",
    "bronze_path = \"s3a://bronze/customer_raw\"\n",
    "\n",
    "# Write to MinIO\n",
    "df_customer.write.mode(\"overwrite\").parquet(bronze_path)\n",
    "\n",
    "print(f\"Data written to {bronze_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step 3: Transform (Bronze to Silver)\n",
    "\n",
    "Let's perform a simple transformation. We'll read the data back from Bronze, filter for active users, and select specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Bronze\n",
    "df_bronze = spark.read.parquet(bronze_path)\n",
    "\n",
    "# Transformation: Filter active customers (active=1) and select key fields\n",
    "df_silver = df_bronze.filter(\"active = 1\") \\\n",
    "    .select(\"customer_id\", \"first_name\", \"last_name\", \"email\", \"create_date\")\n",
    "\n",
    "print(f\"Transformed data has {df_silver.count()} active customers.\")\n",
    "\n",
    "# Write to Silver bucket\n",
    "silver_path = \"s3a://silver/customer_active\"\n",
    "df_silver.write.mode(\"overwrite\").parquet(silver_path)\n",
    "\n",
    "print(f\"Transformed data written to {silver_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Step 4: Verification\n",
    "\n",
    "Let's read back the silver data to verify everything worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verify = spark.read.parquet(silver_path)\n",
    "df_verify.show(10)\n",
    "\n",
    "# Optional: Stop Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have successfully ran a data pipeline that:\n",
    "1.  Extracted data from a transactional DB (Postgres).\n",
    "2.  Loaded it into a raw data lake layer (MinIO Bronze).\n",
    "3.  Transformed it and saved it to a refined layer (MinIO Silver).\n",
    "\n",
    "This is the foundational pattern for modern data engineering platforms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
